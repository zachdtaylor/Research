{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "#Comment to know about Github\n",
    "#Round 1123\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "from __future__ import division\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "board = \"| {0} | {1} | {2} |\\n-------------\\n| {3} | {4} | {5} |\\n-------------\\n| {6} | {7} | {8} |\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(output_dim=30, input_dim=9))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=15))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(output_dim=9))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tie(state):\n",
    "    if 0 not in state and not win(state,1) and not win(state,2): \n",
    "        return True\n",
    "    else: return False\n",
    "    \n",
    "def win(state, token):\n",
    "    if token == state[0] == state[1] == state[2]: return True\n",
    "    if token == state[3] == state[4] == state[5]: return True\n",
    "    if token == state[6] == state[7] == state[8]: return True\n",
    "    if token == state[0] == state[3] == state[6]: return True\n",
    "    if token == state[1] == state[4] == state[7]: return True\n",
    "    if token == state[2] == state[5] == state[8]: return True\n",
    "    if token == state[0] == state[4] == state[8]: return True\n",
    "    if token == state[6] == state[4] == state[2]: return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Computer(object):\n",
    "    def __init__(self):\n",
    "        self.Q = {} # Dictionary will be formatted as {((state), action): q-value}. \n",
    "\n",
    "    def legalRand(self, state): # Returns a random legal move in the state given\n",
    "        possible = []\n",
    "        for i in range(9):\n",
    "            if state[i] == 0:\n",
    "                possible.append(i)\n",
    "        move_index = random.choice(possible)\n",
    "        return move_index\n",
    "    \n",
    "    def legal(self, state): # Returns a list of all legal moves in the state given\n",
    "        legal = []\n",
    "        for i in range(9):\n",
    "            if state[i] == 0:\n",
    "                legal.append(i)\n",
    "        return legal\n",
    "    \n",
    "    def epsilon_greedy(self, epsilon, state): # Makes a random move with probability epsilon\n",
    "        stateA = copy.copy(state)             # Makes the best learned move with probability 1-epsilon\n",
    "        if random.random() > epsilon:\n",
    "            move = self.best_move(state)\n",
    "        else:\n",
    "            move = self.legalRand(state)\n",
    "        stateA[move] = 1\n",
    "        return move, stateA # Returns the move and the new state after having made the move\n",
    "    \n",
    "    def learn(self, games = 10, lrate = 1, discfac = .5, epsilon = .1):\n",
    "        for i in range(games):\n",
    "            state = [0,0,0,0,0,0,0,0,0] \n",
    "            while True:\n",
    "                move, stateA = self.epsilon_greedy(epsilon, state)\n",
    "                if win(stateA, 1): # The agent is rewarded if it wins\n",
    "                    state = tuple(state)\n",
    "                    if (state,move) in self.Q: self.Q[(state,move)] += lrate*(100+discfac*(100)-self.Q[(state,move)]) \n",
    "                    if (state,move) not in self.Q: self.Q[(state,move)] = 0 \n",
    "                    break\n",
    "                if 0 not in stateA: # The agent is punished if the game ends up in a tie\n",
    "                    state = tuple(state)\n",
    "                    if (state,move) in self.Q: self.Q[(state,move)] += lrate*(0+discfac*(-100)-self.Q[(state,move)])\n",
    "                    if (state,move) not in self.Q: self.Q[(state,move)] = 0 \n",
    "                    break\n",
    "                else:\n",
    "                    randmove = self.legalRand(stateA)\n",
    "                    stateA[randmove] = 2\n",
    "                    if win(stateA, 2): # The agent is also punished if the opponent wins\n",
    "                        state = tuple(state)\n",
    "                        if (state,move) in self.Q: self.Q[(state,move)] += lrate*(0+discfac*(-100)-self.Q[(state,move)]) \n",
    "                        if (state,move) not in self.Q: self.Q[(state,move)] = 0 \n",
    "                        break\n",
    "                    else: # Otherwise, updates to Q values are made normally\n",
    "                        state = tuple(state)\n",
    "                        if (state,move) in self.Q: self.Q[(state,move)] += lrate*(0+discfac*(max(self.nextQs(stateA)))-self.Q[(state,move)])\n",
    "                        if (state,move) not in self.Q: self.Q[(state,move)] = 0\n",
    "                state = stateA\n",
    "        print(\"Done.\")\n",
    "        \n",
    "    def nextQs(self, state): # Returns a list of Q values associated with each possible move in the state given\n",
    "        possible_moves, q = self.legal(state), []\n",
    "        state = tuple(state)\n",
    "        for i in possible_moves:\n",
    "            if (state,i) in self.Q: q.append(self.Q[(state,i)])\n",
    "            else: q.append(0)\n",
    "        return q\n",
    "    \n",
    "    def best_move(self, state):\n",
    "        possible_moves, q = self.legal(state), self.nextQs(state)\n",
    "        count = q.count(max(q))\n",
    "        if count > 1: # If there is more than one best move, randomly choose one\n",
    "            best_choices = [k for k in range(len(possible_moves)) if q[k] == max(q)]\n",
    "            move_index = random.choice(best_choices)\n",
    "        else: move_index = q.index(max(q)) # Otherwise, choose the best option\n",
    "        return possible_moves[move_index]\n",
    "        \n",
    "    def play(self, state):\n",
    "        move = self.best_move(state)\n",
    "        print(\"Computer's move: {0}\".format(move+1))\n",
    "        return move\n",
    "    \n",
    "    def getType(self):\n",
    "        return \"Computer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "C = Computer()\n",
    "C.learn(games = 225000, lrate = .2, discfac = 1, epsilon = .1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AI:\n",
    "    def __init__(self):\n",
    "        self.model = model\n",
    "\n",
    "    def legalRand(self, state): # Returns a random legal move in the state given\n",
    "        possible = []\n",
    "        for i in range(9):\n",
    "            if state[i] == 0:\n",
    "                possible.append(i)\n",
    "        move_index = random.choice(possible)\n",
    "        return move_index\n",
    "    \n",
    "    def legal(self, state): # Returns a list of all legal moves in the state given\n",
    "        legal = []\n",
    "        for i in range(9):\n",
    "            if state[i] == 0:\n",
    "                legal.append(i)\n",
    "        return legal\n",
    "    \n",
    "    # Returns max legal q value\n",
    "    def legalQ(self, Qvals, state):\n",
    "        q = []\n",
    "        legal_moves = self.legal(state)\n",
    "        for i in legal_moves: q.append(Qvals[0][i])\n",
    "        return max(q)\n",
    "    \n",
    "    # Makes a random move with probability epsilon\n",
    "    # Makes the best learned move with probability 1-epsilon\n",
    "    def epsilon_greedy(self, epsilon, state):\n",
    "        stateA = copy.copy(state)         \n",
    "        if random.random() > epsilon:\n",
    "            move = self.best_move(state)\n",
    "        else:\n",
    "            move = self.legalRand(state)\n",
    "        stateA[move] = 1\n",
    "        return move, stateA # Returns the move and the new state after having made the move\n",
    "    \n",
    "    def learn(self, games = 10, discfac = .5, epsilon = .1):\n",
    "        games_won = 0\n",
    "        counter = 0\n",
    "        \n",
    "        for i in range(games):\n",
    "            # Each game starts with an empty board\n",
    "            state = np.array([0,0,0,0,0,0,0,0,0])\n",
    "            \n",
    "            while True:\n",
    "                # Get the NN's output for the current state\n",
    "                qvals = model.predict(state.reshape(1,9), verbose=0)\n",
    "                \n",
    "                # Make the best move (highest Q value) with probability 1-epsilon\n",
    "                # and get the new state after move is made\n",
    "                move, new_state = self.epsilon_greedy(epsilon, state)\n",
    "                \n",
    "                # If the AI wins after this move, or causes a tie, break out of the loop\n",
    "                # to train NN\n",
    "                if win(new_state, 1) or tie(new_state): break\n",
    "                \n",
    "                # If we're at this point, the AI didn't win and didn't tie, so\n",
    "                # the opponent makes its move\n",
    "                #if (random.random() >= .5): opp_move = self.legalRand(new_state)\n",
    "                opp_move = C.best_move(new_state)\n",
    "                \n",
    "                # Put a 2 in the state where the opponent made their move\n",
    "                new_state[opp_move] = 2\n",
    "                # Then if the opponent wins, break to train NN\n",
    "                if win(new_state, 2): break\n",
    "                \n",
    "                # By this point, we know neither the AI or opponent has won,\n",
    "                # so we train NN \n",
    "                else:\n",
    "                    # Reward: 0 since game isn't over\n",
    "                    reward = self.getReward(new_state)\n",
    "                    # Get q values on the state now with the new moves made\n",
    "                    newQ = model.predict(new_state.reshape(1,9), verbose=0)\n",
    "                    # Get max legal q value\n",
    "                    maxQ = self.legalQ(newQ, new_state)\n",
    "                    \n",
    "                    # vector y = qvals, differing only in the spot where the AI\n",
    "                    # chose to move. There, y holds discfac * maxQ. The purpose of this\n",
    "                    # is to train each move based on the best option the AI has in \n",
    "                    # the next state. In other words, moves get judged based on future\n",
    "                    # consequences \n",
    "                    update = reward + (discfac * maxQ)\n",
    "                    y = np.zeros((1,9))\n",
    "                    y[:] = qvals[:] \n",
    "                    y[0][move] = update\n",
    "                    #print(\"Move: %r\" % move)\n",
    "                    #print(\"Update: %r\" % update)\n",
    "                    #print(y)\n",
    "                    # 10 epochs to make sure the behavior is enforced \n",
    "                    self.model.fit(qvals, y, nb_epoch=1, verbose=0)\n",
    "                    # set the current state to the new state\n",
    "                    state = new_state\n",
    "            \n",
    "            # Rewards: win = 10, loss = -10, tie = -10\n",
    "            reward = self.getReward(new_state)\n",
    "            # When the game is over, we train soley based on whether the AI won, lost, or tie\n",
    "            y = np.zeros((1,9))\n",
    "            y[:] = qvals[:]\n",
    "            y[0][move] = reward\n",
    "            #print(\"Move: %r\" % move)\n",
    "            #print(\"Reward %s\" % reward)\n",
    "            #print(y)\n",
    "            model.fit(qvals, y, nb_epoch=1, verbose=0)\n",
    "            \n",
    "            # As training progresses, we make it more likely that AI chooses the best moves\n",
    "            if epsilon > .1: epsilon -= 1/games\n",
    "            if (reward == 10): games_won += 1\n",
    "            counter += 1\n",
    "            if counter % 10000 == 0: \n",
    "                print (\"Game {0} of {1} complete.\".format(counter, games))\n",
    "                print(\"Epsilon: \", epsilon)\n",
    "        print (\"Done. Won {0} of {1} games.\".format(games_won, games))\n",
    "    \n",
    "    def getReward(self, state):\n",
    "        if win(state, 1):\n",
    "            return 10\n",
    "        elif win(state, 2) or tie(state):\n",
    "            return -10\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    # Returns a list of Q values associated with each possble move in the state given\n",
    "    def nextQs(self, state): \n",
    "        state = np.asarray(state)\n",
    "        Q, q = model.predict_proba(state.reshape(1,9), verbose=0), []\n",
    "        legal_moves = self.legal(state)\n",
    "        for i in legal_moves: q.append(Q[0][i])\n",
    "        return q\n",
    "    \n",
    "    def best_move(self, state):\n",
    "        legal_moves, q = self.legal(state), self.nextQs(state)\n",
    "        #print(\"Legal moves: {0} q: {1}\".format(legal_moves, q))\n",
    "        count = q.count(max(q))\n",
    "        if count > 1:\n",
    "            best_choices = [k for k in range(len(legal_moves)) if q[k] == max(q)]\n",
    "            move_index = random.choice(best_choices)\n",
    "        else: move_index = q.index(max(q))\n",
    "        #print \"Move_index: %d\" % move_index\n",
    "        return legal_moves[move_index]\n",
    "        \n",
    "    def play(self, state):\n",
    "        move = self.best_move(state)\n",
    "        print (\"Computer's move: {0}\".format(move+1))\n",
    "        return move\n",
    "    \n",
    "    def getType(self):\n",
    "        return \"Computer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Human:\n",
    "    def getType(self):\n",
    "        return \"Human\"\n",
    "\n",
    "    def play(self, state):\n",
    "        move = int(input(\"Your move: \"))\n",
    "        if move >0 and move < 10:\n",
    "            return move-1\n",
    "        else:\n",
    "            raise ValueError(\"Entry must be a number between 1 and 9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def play(self, player1, player2):\n",
    "        state = [0 for j in range(9)]\n",
    "        start = [' ' for i in range(9)]\n",
    "        print (board.format(*start), '\\n')\n",
    "        while True:\n",
    "            player1_move = player1.play(state)\n",
    "            start[player1_move], state[player1_move] = 'X', 1\n",
    "            print (board.format(*start), '\\n')\n",
    "            if win(state, 1): \n",
    "                player1_type = player1.getType()\n",
    "                print(\"%s wins!\" % player1_type)\n",
    "                break\n",
    "            if tie(state):\n",
    "                print (\"Tie.\")\n",
    "                break\n",
    "            player2_move = player2.play(state)\n",
    "            start[player2_move], state[player2_move] = 'O', 2\n",
    "            print (board.format(*start), '\\n')\n",
    "            if win(state, 2):\n",
    "                player2_type = player2.getType()\n",
    "                print (\"%s wins!\" % player2_type)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 10000 of 10000 complete.\n",
      "Epsilon:  0\n",
      "Done. Won 6228 of 10000 games.\n"
     ]
    }
   ],
   "source": [
    "CPU = AI()\n",
    "CPU.learn(games = 10000, discfac = .9, epsilon = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   | \n",
      "\n",
      "Computer's move: 7\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "| X |   |   | \n",
      "\n",
      "Your move: 5\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   | O |   |\n",
      "-------------\n",
      "| X |   |   | \n",
      "\n",
      "Computer's move: 4\n",
      "|   |   |   |\n",
      "-------------\n",
      "| X | O |   |\n",
      "-------------\n",
      "| X |   |   | \n",
      "\n",
      "Your move: 1\n",
      "| O |   |   |\n",
      "-------------\n",
      "| X | O |   |\n",
      "-------------\n",
      "| X |   |   | \n",
      "\n",
      "Computer's move: 3\n",
      "| O |   | X |\n",
      "-------------\n",
      "| X | O |   |\n",
      "-------------\n",
      "| X |   |   | \n",
      "\n",
      "Your move: 9\n",
      "| O |   | X |\n",
      "-------------\n",
      "| X | O |   |\n",
      "-------------\n",
      "| X |   | O | \n",
      "\n",
      "Human wins!\n"
     ]
    }
   ],
   "source": [
    "Me = Human()\n",
    "T = TicTacToe()\n",
    "T.play(CPU, Me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.99997973e-01,   9.37753399e-20,   1.25501457e-13,\n",
       "          6.98801509e-19,   9.99997914e-01,   6.43704715e-19,\n",
       "          1.05167153e-07,   5.96546812e-08,   9.99997854e-01]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([0,0,0,2,0,1,0,2,1]).reshape(1,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
