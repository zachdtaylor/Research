{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from __future__ import division\n",
    "import time\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "board = \"| {0} | {1} | {2} |\\n-------------\\n| {3} | {4} | {5} |\\n-------------\\n| {6} | {7} | {8} |\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(output_dim=30, init=\"zero\", input_dim=9))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=15, init=\"zero\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(output_dim=9, init=\"zero\"))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tie(state):\n",
    "    if 0 not in state and not win(state,1) and not win(state,2): \n",
    "        return True\n",
    "    else: return False\n",
    "    \n",
    "def win(state, token):\n",
    "    if token == state[0] == state[1] == state[2]: return True\n",
    "    if token == state[3] == state[4] == state[5]: return True\n",
    "    if token == state[6] == state[7] == state[8]: return True\n",
    "    if token == state[0] == state[3] == state[6]: return True\n",
    "    if token == state[1] == state[4] == state[7]: return True\n",
    "    if token == state[2] == state[5] == state[8]: return True\n",
    "    if token == state[0] == state[4] == state[8]: return True\n",
    "    if token == state[6] == state[4] == state[2]: return True\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QTable(object):   #This class is a Q-table learning algorithm\n",
    "    def __init__(self):\n",
    "        self.Q = {} # Dictionary will be formatted as {((state), action): q-value}. \n",
    "\n",
    "    def legalRand(self, state): # Returns a random legal move in the state given\n",
    "        possible = []\n",
    "        for i in range(9):\n",
    "            if state[i] == 0:\n",
    "                possible.append(i)\n",
    "        move_index = random.choice(possible)\n",
    "        return move_index\n",
    "    \n",
    "    def legal(self, state): # Returns a list of all legal moves in the state given\n",
    "        legal = []\n",
    "        for i in range(9):\n",
    "            if state[i] == 0:\n",
    "                legal.append(i)\n",
    "        return legal\n",
    "    \n",
    "    def epsilon_greedy(self, epsilon, state): # Makes a random move with probability epsilon\n",
    "        stateA = copy.copy(state)             # Makes the best learned move with probability 1-epsilon\n",
    "        if random.random() > epsilon:\n",
    "            move = self.best_move(state)\n",
    "        else:\n",
    "            move = self.legalRand(state)\n",
    "        stateA[move] = 1\n",
    "        return move, stateA # Returns the move and the new state after having made the move\n",
    "    \n",
    "    def learn(self, games = 10, lrate = 1, discfac = .5, epsilon = .1):\n",
    "        for i in range(games):\n",
    "            state = [0,0,0,0,0,0,0,0,0] \n",
    "            while True:\n",
    "                move, stateA = self.epsilon_greedy(epsilon, state)\n",
    "                if win(stateA, 1): # The agent is rewarded if it wins\n",
    "                    state = tuple(state)\n",
    "                    if (state,move) in self.Q: self.Q[(state,move)] += lrate*(100+discfac*(100)-self.Q[(state,move)]) \n",
    "                    if (state,move) not in self.Q: self.Q[(state,move)] = 0 \n",
    "                    break\n",
    "                if 0 not in stateA: # The agent is rewarded lightly if game ends in a tie\n",
    "                    state = tuple(state)\n",
    "                    if (state,move) in self.Q: self.Q[(state,move)] += lrate*(0+discfac*(10)-self.Q[(state,move)])\n",
    "                    if (state,move) not in self.Q: self.Q[(state,move)] = 0 \n",
    "                    break\n",
    "                else:\n",
    "                    randmove = self.legalRand(stateA)\n",
    "                    stateA[randmove] = 2\n",
    "                    if win(stateA, 2): # The agent is punished if the opponent wins\n",
    "                        state = tuple(state)\n",
    "                        if (state,move) in self.Q: self.Q[(state,move)] += lrate*(0+discfac*(-100)-self.Q[(state,move)]) \n",
    "                        if (state,move) not in self.Q: self.Q[(state,move)] = 0 \n",
    "                        break\n",
    "                    else: # Otherwise, updates to Q values are made normally\n",
    "                        state = tuple(state)\n",
    "                        if (state,move) in self.Q: self.Q[(state,move)] += lrate*(0+discfac*(max(self.nextQs(stateA)))-self.Q[(state,move)])\n",
    "                        if (state,move) not in self.Q: self.Q[(state,move)] = 0\n",
    "                state = stateA\n",
    "        print(\"Done.\")\n",
    "        \n",
    "    def nextQs(self, state): # Returns a list of Q values associated with each possible move in the state given\n",
    "        possible_moves, q = self.legal(state), []\n",
    "        state = tuple(state)\n",
    "        for i in possible_moves:\n",
    "            if (state,i) in self.Q: q.append(self.Q[(state,i)])\n",
    "            else: q.append(0)\n",
    "        return q\n",
    "    \n",
    "    def best_move(self, state):\n",
    "        possible_moves, q = self.legal(state), self.nextQs(state)\n",
    "        count = q.count(max(q))\n",
    "        if count > 1: # If there is more than one best move, randomly choose one\n",
    "            best_choices = [k for k in range(len(possible_moves)) if q[k] == max(q)]\n",
    "            move_index = random.choice(best_choices)\n",
    "        else: move_index = q.index(max(q)) # Otherwise, choose the best option\n",
    "        return possible_moves[move_index]\n",
    "        \n",
    "    def play(self, state):\n",
    "        move = self.best_move(state)\n",
    "        print(\"Computer's move: {0}\".format(move+1))\n",
    "        return move\n",
    "    \n",
    "    def getType(self):\n",
    "        return \"QTable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
<<<<<<< HEAD
      "Time:  26.93854284286499\n"
=======
      "Time: 21.19906997680664\n"
>>>>>>> 9fb484d81157f969958f45c32324fc4cac686b49
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "Q_learner = QTable()\n",
    "start = time.time()\n",
    "Q_learner.learn(games = 400000, lrate = .1, discfac = 1, epsilon = .1)\n",
    "end = time.time()\n",
    "print(\"Time: \", end-start)"
=======
    "C = QTable()\n",
    "start = time.time()\n",
    "C.learn(games = 300000, lrate = .1, discfac = 1, epsilon = .1) \n",
    "end = time.time()\n",
    "print(\"Time: {0}\".format(end-start))"
>>>>>>> 9fb484d81157f969958f45c32324fc4cac686b49
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:    #This class is a neural network Q-learning algorithm\n",
    "    def __init__(self):\n",
    "        self.model = model\n",
    "\n",
    "    def legalRand(self, state): # Returns a random legal move in the state given\n",
    "        possible = []\n",
    "        for i in range(9):\n",
    "            if state[i] == 0:\n",
    "                possible.append(i)\n",
    "        move_index = random.choice(possible)\n",
    "        return move_index\n",
    "    \n",
    "    def legal(self, state): # Returns a list of all legal moves in the state given\n",
    "        legal = []\n",
    "        for i in range(9):\n",
    "            if state[i] == 0:\n",
    "                legal.append(i)\n",
    "        return legal\n",
    "    \n",
    "    # Returns max legal q value\n",
    "    def legalQ(self, Qvals, state):\n",
    "        q = []\n",
    "        legal_moves = self.legal(state)\n",
    "        for i in legal_moves: q.append(Qvals[0][i])\n",
    "        return max(q)\n",
    "    \n",
    "    # Makes a random move with probability epsilon\n",
    "    # Makes the best learned move with probability 1-epsilon\n",
    "    def epsilon_greedy(self, epsilon, state):\n",
    "        stateA = copy.copy(state)         \n",
    "        if random.random() > epsilon:\n",
    "            move = self.best_move(state)\n",
    "        else:\n",
    "            move = self.legalRand(state)\n",
    "        stateA[move] = 1\n",
    "        return move, stateA # Returns the move and the new state after having made the move\n",
    "    \n",
    "    def learn(self, games, discfac, epsilon):\n",
    "        games_won = 0\n",
    "        counter = 0\n",
    "        \n",
    "        for i in range(games):\n",
    "            # Each game starts with an empty board\n",
    "            state = np.array([0,0,0,0,0,0,0,0,0])\n",
    "            \n",
    "            while True:\n",
    "                # Get the NN's output for the current state\n",
    "                qvals = model.predict(state.reshape(1,9), verbose=0)\n",
    "                \n",
    "                # Make the best move (highest Q value) with probability 1-epsilon\n",
    "                # and get the new state after move is made\n",
    "                move, new_state = self.epsilon_greedy(epsilon, state)\n",
    "                \n",
    "                # If the NeuralNetwork wins after this move, or causes a tie, break out of the loop\n",
    "                # to train NN\n",
    "                if win(new_state, 1):\n",
    "                    games_won += 1\n",
    "                    break\n",
    "                    \n",
    "                if tie(new_state): break\n",
    "                \n",
    "                # If we're at this point, the NeuralNetwork didn't win and didn't tie, so\n",
    "                # the opponent makes its move\n",
    "                #if (random.random() >= .5): opp_move = self.legalRand(new_state)\n",
    "                opp_move = Q_learner.best_move(new_state)\n",
    "                \n",
    "                # Put a 2 in the state where the opponent made their move\n",
    "                new_state[opp_move] = 2\n",
    "                # Then if the opponent wins, break to train NN\n",
    "                if win(new_state, 2): break\n",
    "                \n",
    "                # By this point, we know neither the NeuralNetwork or opponent has won,\n",
    "                # so we train NN \n",
    "                else:\n",
    "                    # Reward: 0 since game isn't over\n",
    "                    reward = self.getReward(new_state)\n",
    "                    # Get q values on the state now with the new moves made\n",
    "                    newQ = model.predict(new_state.reshape(1,9), verbose=0)\n",
    "                    # Get max legal q value\n",
    "                    maxQ = self.legalQ(newQ, new_state)\n",
    "                    \n",
    "                    # vector y = qvals, differing only in the spot where the NeuralNetwork\n",
    "                    # chose to move. There, y holds discfac * maxQ. The purpose of this\n",
    "                    # is to train each move based on the best option the NeuralNetwork has in \n",
    "                    # the next state. In other words, moves get judged based on future\n",
    "                    # consequences \n",
    "                    update = reward + (discfac * maxQ)\n",
    "                    y = np.zeros((1,9))\n",
    "                    y[:] = qvals[:] \n",
    "                    y[0][move] = update\n",
    "                    #print(\"Move: %r\" % move)\n",
    "                    #print(\"Update: %r\" % update)\n",
    "                    #print(y)\n",
    "                    # 10 epochs to make sure the behavior is enforced \n",
    "                    self.model.fit(qvals, y, nb_epoch=1, verbose=0)\n",
    "                    # set the current state to the new state\n",
    "                    state = new_state\n",
    "            \n",
    "            # Rewards: win = 10, loss = -10, tie = -10\n",
    "            reward = self.getReward(new_state)\n",
    "            # When the game is over, we train soley based on whether the NeuralNetwork won, lost, or tie\n",
    "            y = np.zeros((1,9))\n",
    "            y[:] = qvals[:]\n",
    "            y[0][move] = reward\n",
    "            #print(\"Move: %r\" % move)\n",
    "            #print(\"Reward %s\" % reward)\n",
    "            #print(y)\n",
    "            model.fit(qvals, y, nb_epoch=1, verbose=0)\n",
    "            \n",
    "            # As training progresses, we make it more likely that NeuralNetwork chooses the best moves\n",
    "            if epsilon > .1: epsilon -= 1/games\n",
    "            #if (reward == 10): games_won += 1\n",
    "            counter += 1\n",
    "            if counter % 10000 == 0: \n",
    "                print (\"Game {0} of {1} complete.\".format(counter, games))\n",
    "                print(\"Epsilon: \", epsilon)\n",
    "        print (\"Done. Won {0} of {1} games.\".format(games_won, games))\n",
    "    \n",
    "    def learnWithQ(self, games, discfac, epsilon, Qplayer):\n",
    "        counter = 0\n",
    "        games_won = 0\n",
    "        for i in range(games):\n",
    "            # Each game starts with an empty board\n",
    "            state = np.array([0,0,0,0,0,0,0,0,0])\n",
    "            j= 0\n",
    "            while True:\n",
    "                reward = 0\n",
    "                # Get the NN's output for the current state\n",
    "                qvals = model.predict(state.reshape(1,9), verbose=0)\n",
    "                \n",
    "                # Make the best move (highest Q value) with probability 1-epsilon\n",
    "                # and get the new state after move is made\n",
    "                move, new_state = self.epsilon_greedy(epsilon, state)\n",
    "                #print(new_state)\n",
    "                # If the NeuralNetwork wins after this move, or causes a tie, break out of the loop\n",
    "                # to train NN\n",
    "                if win(new_state, 1):\n",
    "                    #print(\"win\", j)\n",
    "                    reward = 10\n",
    "                    games_won += 1\n",
    "                    break\n",
    "                    \n",
    "                if tie(new_state): \n",
    "                    #print(\"tie\")\n",
    "                    games_won +=1\n",
    "                    break\n",
    "                \n",
    "                # If we're at this point, the NeuralNetwork didn't win and didn't tie, so\n",
    "                # the opponent makes its move\n",
    "                if (random.random() >= 1):\n",
    "                    opp_move = Qplayer.best_move(new_state)\n",
    "                else:\n",
    "                    opp_move = self.legalRand(new_state)\n",
    "                \n",
    "                # Put a 2 in the state where the opponent made their move\n",
    "                new_state[opp_move] = 2\n",
    "                #print(new_state)\n",
    "                # Then if the opponent wins, break to train NN\n",
    "                if win(new_state, 2): \n",
    "                    #print(\"loss\")\n",
    "                    reward = -10\n",
    "                    break\n",
    "                \n",
    "                # By this point, we know neither the NeuralNetwork or opponent has won,\n",
    "                # so we train NN \n",
    "                else:\n",
    "                    # Reward: 0 since game isn't over\n",
    "                    # Get q values on the state now with the new moves made\n",
    "                    newQ = model.predict(new_state.reshape(1,9), verbose=0)\n",
    "                    # Get max legal q value\n",
    "                    maxQ = self.legalQ(newQ, new_state)\n",
    "                    \n",
    "                    # vector y = qvals, differing only in the spot where the NeuralNetwork\n",
    "                    # chose to move. There, y holds discfac * maxQ. The purpose of this\n",
    "                    # is to train each move based on the best option the NeuralNetwork has in \n",
    "                    # the next state. In other words, moves get judged based on future\n",
    "                    # consequences \n",
    "                    update = (discfac * maxQ)\n",
    "                    y = np.zeros((1,9))\n",
    "                    y[:] = qvals[:] \n",
    "                    y[0][move] = update\n",
    "                    #print(\"Move: %r\" % move)\n",
    "                    #print(\"Update: %r\" % update)\n",
    "                    #print(y)\n",
    "                    # 10 epochs to make sure the behavior is enforced \n",
    "                    self.model.fit(qvals, y, nb_epoch=1, verbose=0)\n",
    "                    # set the current state to the new state\n",
    "                    state = new_state\n",
    "                    j+=1\n",
    "            \n",
    "            # Rewards: win = 10, loss = -10, tie = -10\n",
    "            # When the game is over, we train soley based on whether the NeuralNetwork won, lost, or tie\n",
    "            y = np.zeros((1,9))\n",
    "            y[:] = qvals[:]\n",
    "            y[0][move] = 2*reward\n",
    "            #print(\"Move: %r\" % move)\n",
    "            #print(\"Reward %s\" % reward)\n",
    "            #print(y)\n",
    "            model.fit(qvals, y, nb_epoch=1, verbose=0)\n",
    "            \n",
    "            # As training progresses, we make it more likely that NeuralNetwork chooses the best moves\n",
    "            if epsilon > .1: epsilon -= 1/games\n",
    "            #if (reward == 10): games_won += 1\n",
    "            counter += 1\n",
    "            if counter % 10000 == 0: \n",
    "                print (\"Game {0} of {1} complete.\".format(counter, games))\n",
    "                print(\"Epsilon: \", epsilon)\n",
    "        print (\"Done. Won {0} of {1} games.\".format(games_won, games))\n",
    "\n",
    "    \n",
    "    def getReward(self, state):\n",
    "        if win(state, 1):\n",
    "            return 10\n",
    "        elif win(state, 2):\n",
    "            return -10\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    # Returns a list of Q values associated with each possble move in the state given\n",
    "    def nextQs(self, state): \n",
    "        state = np.asarray(state)\n",
    "        Q, q = model.predict_proba(state.reshape(1,9), verbose=0), []\n",
    "        legal_moves = self.legal(state)\n",
    "        for i in legal_moves: q.append(Q[0][i])\n",
    "        return q\n",
    "    \n",
    "    def best_move(self, state):\n",
    "        legal_moves, q = self.legal(state), self.nextQs(state)\n",
    "        #print(\"Legal moves: {0} q: {1}\".format(legal_moves, q))\n",
    "        count = q.count(max(q))\n",
    "        if count > 1:\n",
    "            best_choices = [k for k in range(len(legal_moves)) if q[k] == max(q)]\n",
    "            move_index = random.choice(best_choices)\n",
    "        else: move_index = q.index(max(q))\n",
    "        #print \"Move_index: %d\" % move_index\n",
    "        return legal_moves[move_index]\n",
    "        \n",
    "    def play(self, state):\n",
    "        move = self.best_move(state)\n",
    "        print (\"Computer's move: {0}\".format(move+1))\n",
    "        return move\n",
    "    \n",
    "    def getType(self):\n",
    "        return \"Computer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Human:\n",
    "    def getType(self):\n",
    "        return \"Human\"\n",
    "\n",
    "    def play(self, state):\n",
    "        move = int(input(\"Your move: \"))\n",
    "        if move >0 and move < 10:\n",
    "            return move-1\n",
    "        else:\n",
    "            raise ValueError(\"Entry must be a number between 1 and 9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def play(self, player1, player2):\n",
    "        state = [0 for j in range(9)]\n",
    "        start = [' ' for i in range(9)]\n",
    "        print (board.format(*start), '\\n')\n",
    "        while True:\n",
    "            player1_move = player1.play(state)\n",
    "            start[player1_move], state[player1_move] = 'X', 1\n",
    "            print (board.format(*start), '\\n')\n",
    "            if win(state, 1): \n",
    "                player1_type = player1.getType()\n",
    "                print(\"%s wins!\" % player1_type)\n",
    "                break\n",
    "            if tie(state):\n",
    "                print (\"Tie.\")\n",
    "                break\n",
    "            player2_move = player2.play(state)\n",
    "            start[player2_move], state[player2_move] = 'O', 2\n",
    "            print (board.format(*start), '\\n')\n",
    "            if win(state, 2):\n",
    "                player2_type = player2.getType()\n",
    "                print (\"%s wins!\" % player2_type)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Game 10000 of 150000 complete.\n",
      "Epsilon:  0\n",
      "Game 20000 of 150000 complete.\n",
      "Epsilon:  0\n",
      "Game 30000 of 150000 complete.\n",
      "Epsilon:  0\n",
      "Game 40000 of 150000 complete.\n",
      "Epsilon:  0\n",
      "Game 50000 of 150000 complete.\n",
      "Epsilon:  0\n",
      "Game 60000 of 150000 complete.\n",
      "Epsilon:  0\n",
      "Game 70000 of 150000 complete.\n",
      "Epsilon:  0\n",
      "Game 80000 of 150000 complete.\n",
      "Epsilon:  0\n",
      "Game 90000 of 150000 complete.\n",
      "Epsilon:  0\n",
      "Game 100000 of 150000 complete.\n",
      "Epsilon:  0\n",
      "Game 110000 of 150000 complete.\n",
      "Epsilon:  0\n",
      "Game 120000 of 150000 complete.\n",
      "Epsilon:  0\n",
      "Game 130000 of 150000 complete.\n",
      "Epsilon:  0\n",
      "Game 140000 of 150000 complete.\n",
      "Epsilon:  0\n",
      "Game 150000 of 150000 complete.\n",
      "Epsilon:  0\n",
      "Done. Won 131840 of 150000 games.\n"
=======
      "Game 10000 of 10000 complete.\n",
      "Epsilon:  0\n",
      "Done. Won 8423 of 10000 games.\n",
      "time: 26.52942991256714\n"
>>>>>>> 9fb484d81157f969958f45c32324fc4cac686b49
     ]
    }
   ],
   "source": [
    "CPU = NeuralNetwork()\n",
<<<<<<< HEAD
    "CPU.learnWithQ(games = 150000, discfac = .9, epsilon = 0, Qplayer = Q_learner)"
=======
    "start = time.time()\n",
    "CPU.learn(games = 10000, discfac = .9, epsilon = 0)\n",
    "end = time.time()\n",
    "print(\"time: {0}\".format(end-start))"
>>>>>>> 9fb484d81157f969958f45c32324fc4cac686b49
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 19,
>>>>>>> 9fb484d81157f969958f45c32324fc4cac686b49
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   | \n",
      "\n",
<<<<<<< HEAD
      "Computer's move: 9\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   | X | \n",
      "\n",
      "Your move: 1\n",
      "| O |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   | X | \n",
      "\n",
      "Computer's move: 5\n",
      "| O |   |   |\n",
      "-------------\n",
      "|   | X |   |\n",
      "-------------\n",
      "|   |   | X | \n",
      "\n",
      "Your move: 2\n",
      "| O | O |   |\n",
      "-------------\n",
      "|   | X |   |\n",
      "-------------\n",
      "|   |   | X | \n",
      "\n",
      "Computer's move: 3\n",
      "| O | O | X |\n",
=======
      "Computer's move: 2\n",
      "|   | X |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   | \n",
      "\n",
      "Your move: 1\n",
      "| O | X |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   | \n",
      "\n",
      "Computer's move: 3\n",
      "| O | X | X |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   | \n",
      "\n",
      "Your move: 5\n",
      "| O | X | X |\n",
>>>>>>> 9fb484d81157f969958f45c32324fc4cac686b49
      "-------------\n",
      "|   | O |   |\n",
      "-------------\n",
<<<<<<< HEAD
      "|   |   | X | \n",
      "\n",
      "Your move: 6\n",
      "| O | O | X |\n",
      "-------------\n",
      "|   | X | O |\n",
      "-------------\n",
      "|   |   | X | \n",
      "\n",
      "Computer's move: 7\n",
      "| O | O | X |\n",
      "-------------\n",
      "|   | X | O |\n",
      "-------------\n",
      "| X |   | X | \n",
      "\n",
      "Computer wins!\n"
=======
      "|   |   |   | \n",
      "\n",
      "Computer's move: 7\n",
      "| O | X | X |\n",
      "-------------\n",
      "|   | O |   |\n",
      "-------------\n",
      "| X |   |   | \n",
      "\n",
      "Your move: 9\n",
      "| O | X | X |\n",
      "-------------\n",
      "|   | O |   |\n",
      "-------------\n",
      "| X |   | O | \n",
      "\n",
      "Human wins!\n"
>>>>>>> 9fb484d81157f969958f45c32324fc4cac686b49
     ]
    }
   ],
   "source": [
    "Me = Human()\n",
    "T = TicTacToe()\n",
    "T.play(CPU, Me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.97872853,  0.01972129,  0.24087605,  0.0654934 ,  0.97876471,\n",
       "         0.22053277,  0.12109259,  0.02937374,  0.97877759]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([0,0,0,2,0,1,0,2,1]).reshape(1,9))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
